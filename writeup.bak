\documentclass{article}

\usepackage{amsthm, amsmath, amsfonts}

\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\tikzset{main node/.style={circle,draw,minimum size=5mm,inner sep=0pt}}
\tikzset{edge/.style = {->,> = latex'}}
\tikzset{eq/.style={rounded corners=8,dashed}}

\usepackage[utf8]{inputenc}

\usepackage{algorithm, algorithmicx}
\usepackage[noend]{algpseudocode}
\newcommand*\Let[2]{\State #1 $\gets$ #2}
\newcommand*\Letn[2]{; #1 $\gets$ #2}

\usepackage{natbib}
\bibliographystyle{unsrtnat}

\pdfpageheight 11in
\pdfpagewidth 8.5in
\topmargin -1in
\headheight 0in
\headsep 0in
\textheight 8.5in
\textwidth 6.5in
\oddsidemargin 0in
\evensidemargin 0in
\headheight 77pt
\headsep 0in
\footskip .75in

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{fact}{Fact}

\usepackage{mathtools}
\DeclarePairedDelimiter\p{(}{)}
\DeclarePairedDelimiter\abs{|}{|}
\DeclarePairedDelimiter\br{[}{]}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\ip{\langle}{\rangle}
\DeclarePairedDelimiter\set{\{}{\}}

\title{Precedence Constraints in Energy Aware Scheduling}
\author{Benjamin Kuykendall (brk2117), Steven Shao (ys2833)}
\date{16 December 2016}

\begin{document}

\maketitle

\begin{abstract}
    Energy aware scheduling allows machines to run at varying speeds, but with a power use increasing with speed. Given this new measure, a schedule can optimize for some function of energy and a more traditional scheduling objective. We focus on minimizing energy use under a fixed makespan on identical machines. This is a trivial problem; however, it becomes less so when we add precedence constraints to the jobs. Precedence constraints can take the form of DAGs, but as an easier step, we consider only trees. We introduce an algorithm for chains and analyze it; we were hoping to prove optimality, but could not get a small piece of the proof. Building on our intuition about this partial result, we propose heuristics for scheduling trees under the same objectives. Using the tool of chain decompositions, along with the guarantees of the chain scheduling algorithm, we create an approximation algorithm for trees and compute a bound for the approximation ratio. We simulate these algorithms and compare quantitative results suggesting which heuristic does best on realistic trees.
\end{abstract}

\section{Introduction}
\subsection{Background}
The topics of energy and precedence were well motivated individually in class. Energy use is of growing importance in models with varying machine speeds, like many modern processors, where maximal speed of course achieves the fastest schedule, but at the cost of larger energy usage. 
Precedence constraints model a variety of real life problems, like the order of steps in a project or industrial process, or the evaluation of an expression tree or functional program.
The conjunction of the two is motivated by the combination of these concerns. It is easy to imagine a computation that must be aware of energy, speed, and order.

In particular, we want to consider the problem $P|prec, C_{max} = C|E$ where precedence-bound jobs must be run in a fixed amount of time and should be scheduled to minimize energy use. This can be thought of as a special case of the release date and deadlines used in class for energy aware scheduling -- all of the jobs have release date $t = 0$ and deadline $t = C$. However, the problem is compelling even without this analogy. Consider the example of an expression tree where each job is to compute a subexpression. By setting a makespan $C$, we are saying that we do not care when the subexpressions are computed, but just want the final result of the full expression at time $C$. So in effect, we are only setting a deadline for the maximal jobs in the partial order.

A quick review of energy aware scheduling shows how solved problems differ from ours. The differences occur in a few directions. First, most works consider jobs with release dates and deadlines \cite{yao}, \cite{irani}, or release dates and flow time \cite{bansal}. To us, these models could be too restrictive in some cases: in applications like the expression trees mentioned above, we do not care when individual jobs are completed, rather we care about a deadline for the top level jobs and we care that jobs are completed in the correct order. Second, some energy aware scheduling algorithms are online and must schedule jobs as they come in \cite{yao}, \cite{irani}. This is an important problem, but it can be considerably harder than the offline case. Third, sometimes a slightly different energy model is used where machines can be idled for some fixed cost \cite{baptiste}. This could be a good direction to extend our work, especially because many precedence graphs require some amount of idling. But again, we ignore idling for the sake of relative simplicity.

Similarly, we should review precedence constraints: most relevantly the problem $P|prec|C_{max}$. The simple algorithm given in 1961 can be simply described as ``highest height first'': in the next slot, always schedule the possible job that is furthest from a sink \cite{hu}. This is an easy and intuitive algorithm, but it is underspecified as how to break ties. Although this does not matter in the makespan minimization problem, we will see how this seemly arbitrary choice changes energy consumption.

\subsection{Notation}
We attempt to be consistent with the notation used in class. Use $n$ to denote the total number of jobs, $m$ the number of machines, and $C_{max}$ the latest completion time or makespan. The variable $j$ will unless otherwise mentioned refer to one of the jobs, which are represented as elements of $[n] := \{1,\dots,n\}$. 

Precedence constraints will be represented as a partial order $j \prec j'$ meaning that job $j$ must be completed before job $j'$ begins. We will often think of the underlying DAG of $\prec$ as a graph. A special case is when $\prec$ can be represented as a chain, a series $a_{n_1}, \dots, a_{n_k}$ such that $a_{n_\alpha} \prec a_{n_\beta}$ if and only if $\alpha < \beta$. These chains are then graphs where the only edges are between adjacent $a_n$.

For energy aware scheduling, we assign each job a quantity of work $w_j > 0$. Then, we choose a speed $s$ for the machine to run at each time, and job $j$ started at time $t_0$ is completed at $t$ where $w_j = \int_{t_0}^t s(t) dt$. The energy used by a machine is defined as $E = \int p(s(t))dt$ for some ``power function'' $p$, where energy adds linearly over machines. We only consider convex increasing power functions, for the reasons discussed in class. From a physical perspective, a realistic power function is $p(s) = s^\alpha$ where $\alpha \approx 3$. Also, for a convex power function, it is never beneficial to change speeds during a job, so, a schedule is uniquely decided by a choice of start time, machine assignment, and duration for each job.

Throughout, we only consider unit work jobs $w_j = 1$. Then, setting $C_{max} = C$, we find the schedule that minimizes energy assuming $m$ identical machines. This problem can be notated $P|prec, C_{max} = C|E$. For our actual implementation, we set $C = 1$, as the actual value should not affect the scheduling decisions made.

% Steven -- we need this?
% L_k^t = length of chain k at start of time t
% |M_i| = load/# jobs scheduled on machine i, M_i

\subsection{Lower Bounds}\label{lb}
As in related works our lower bounds are of two types: congestion and dilation (bounds formulated in \cite{shmoys}, terminology from \cite{kumar}) .
Because we bound $E$ instead of $C_{max}$, the form of our bounds will differ from these works; however, the derivation is identical.

A congestion lower bound is derived by considering the total machine load.
Even ignoring the precedence constraints, we know that in the best case, each job will use time $C\cdot m/n$.
Thus summing over machines and integrating over time, we get a bound on energy.
\begin{equation} \label{eq:lb1}
E \ge C\cdot m\cdot p\left(\frac{n}{C\cdot m}\right).
\end{equation}
A dilation lower bound relates to chains in the graph.
Fixing a chain of length $k$, we know processing the chain will take the least energy if it is allocated a whole machine for the entirety of the schedule.
This gives speed $C/k$ and $E_k \ge C \cdot p(k/C)$.
Because energies add, we can consider any set of disjoint chains of lengths $k_i$ to get a better bound.
\begin{equation} \label{eq:lb2}
E \ge C \sum_i p(k_i/C).
\end{equation}
We do not have a reason to believe that either of these lower bounds, or the maximum of the two, is tight.
However, they are very natural lower bounds and will appear in the analysis of our approximation algorithm and our quantitative analysis.

\subsection{Convex Program}
If the machine assignment and order is already known, notated as a partition of the machines $J_{a,i}$ the $i^{th}$ job run on the $a^{th}$ machine, we can find the optimal schedule with a convex optimization over variables $s_j$ the start time and $r_j$ the runtime of job $j$.
\begin{subequations}
\begin{align}
\label{eq:cp_obj} &\text{minimize } \textstyle\sum_{j} r_j \cdot p(1/r_j) \text{ subject to } \\
\label{eq:cp_pos} &\forall j \le n:\: s_j \ge 0 \text{ and } r_j \ge 0 \\
\label{eq:cp_compl} &\forall j:\: s_j + r_j \le C \\
\label{eq:cp_prec} &\forall j \prec j' :\: s_j + r_j \le s_{j'} \\
\label{eq:cp_one} &\forall a \forall i :\: s_{(J_{a,i})} + r_{(J_{a,i})} \le s_{(J_{a,i+1})}
\end{align}
\end{subequations}
This is a convex program, as the constraints are all linear and the objective is convex because $p$ is convex. To briefly explain: 
\ref{eq:cp_pos} asserts start and run times are non-negative,
\ref{eq:cp_compl} asserts all jobs complete in the allotted makespan,
\ref{eq:cp_prec} asserts the precedence constraints are respected,
\ref{eq:cp_one} asserts no more than one job runs at each time.
Then \ref{eq:cp_obj} is simply the energy, given that speed is work over runtime.

Conceptually, the existence of such a simple program means that if we can generate an ordered assignment, then choosing speeds to respect makespan and precedence while minimizing $E$ should be easy, whether by running out-of-the-box optimization software on this program or by finding an analytic solution to a more specific problem.

\section{Chains}
We begin our investigation with chains, a simpler class of precedence constraints. We say that the precedence constraints form a set of chains if the out-degree and the in-degree are both bounded by 1 (and there are no cycles); in the in-arborescences we consider later, only the out-degree is bounded by 1. Each connected component in the precedence graph is one such ``chain'' and the length of the chain is the number of elements in it.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}
        \node[main node] at (1,0.5) (A1) {A};
        \node[main node] at (2.2,0.5) (A2) {B};
        \node[main node] at (1,1.5) (B1) {C};
        \node[main node] at (2.2,1.5) (B2) {D};
        \node[main node] at (1.6,2.5) (C) {E};
        \draw[edge] (A1) to (A2);
        \draw[edge] (B1) to (B2);
        \end{tikzpicture}
    \caption{An example of chain precedence constraints.}
    \label{fig:chains}
\end{figure}

\subsection{Motivation}
To inspire our algorithm, we consider the problem of minimizing the makespan of chains, notated \[P|chains|C_{max}\]
It is easy to see that a greedy ``longest chain first''---a specific instance of the solution for a tree given in Hu \cite{hu}---solves this problem. At each timestep, we can iterate through each machine and select the job at the top of any chain with maximal length, if there are more chains left than machines; if there are more machines than chains left, and all chains have been ``scheduled'' for this round, a machine can be left idle. A promising first direction would be to start with a makespan-minimal schedule for chains and then scale each machine's schedule in order to get a makespan $C$ minimal energy schedule, as shown in Figure \ref{fig:chain_min}, though it's not immediately obvious if such an approach is correct, let alone optimal.

\begin{figure}[h!]
\begin{center}\input{cmax_chains_ex.tex}\end{center}
\caption{A minimal makespan schedule and its scaled energy equivalent for the chains in Figure \ref{fig:chains}.}
\label{fig:chain_min}
\end{figure}

Indeed, such a rescaling is not always possible. In some cases, an even rescaling violates precedence constraints, even for schedules output from a ``longest chain first'' algorithm. In Figure \ref{fig:chain_fail}, note that an even scaling would cause C and D to overlap, breaking their precedence constraint. This is only possible when a chain is interwoven between machines, as the chains CD and AB are here.

\begin{figure}[h!]
\begin{center}\input{cmax_chains_bad_ex.tex}\end{center}
\caption{Another LCF schedule for the chains in Figure \ref{fig:chains}. An even scaling would violate the precedence constraints in the red region, so we must scale differently. Specifically, the scaling to the right is optimal.}
\label{fig:chain_fail}
\end{figure}

\subsection{Cyclic Rule}
If the machine iteration order and link selection from each chain is done in a specific manner, we can show that this scaling is always viable. 
Here is an outline of Algorithm \ref{alg:cyclic_chain_sched},  a \textit{cyclic rule} for breaking ties in Hu.
\begin{enumerate}
\item Sort chains by descending length, so that initially longer chains have higher \textit{chain priority}. If two chains have the same initial length, chain priority tie-breaking is arbitrary.
\item Number each node in the chains with the dictionary order, first by height on a chain, and then by chain priority. This is a node's \textit{link priority}.
\item Fix an order on the machines and iterate through them in that order each timestep. To select a node to put on a machine, simply select the node with the highest link priority for which no other node in that chain has been selected that timestep.
\end{enumerate}

We notate chains with $C_1, \ldots, C_c$, where $C_1$ has the highest priority and $c$ is the number of chains. In our discussion, we also mention the instant priority of a chain at time $t$, which is just a chain's position in the dictionary order, sorted first by remaining chain length at time $t$, then by the chain priority assigned in step 1. Another way of looking at the assignments is noting that at each machine iteration, we find the chain with the longest remaining length that is yet to be scheduled this timestep, and break ties with chain priority.

\begin{figure}[h!]
\begin{center}\input{cyclic_rule.tex}\end{center}
\caption{Chains, ordered by chain priority, followed by cyclic rule schedule and scaled schedule.}
\label{fig:cyclic_ex}
\end{figure}

This builds on the algorithm given in \cite{hu} that specifies essentially how to break ties in chain and machine selection, because such ties matter after scaling. At each step we are still selecting from a longest chain first.

\begin{algorithm}[h!]
    \caption{Algorithm for $P|chains, C_{max} = C|E$}
    \label{alg:cyclic_chain_sched}
    \input{cyclic_chain_sched.alg}
\end{algorithm}

\subsection{Viability}
To show that scaling is always viable using the cyclic rule, it is important to note some structural properties of the schedules before scaling. The first is that if the machines are in order $M_1, \ldots, M_m$, then the number of jobs on $M_i$ is at least as many as the number of jobs on machine $M_{i+1}$, so that no timeslice has any ``holes'' in it. Similarly, no machine schedule has any holes in it; if a job is assigned to $M_i$ at time $t$, then for every $t' < t$, a job must have been assigned to machine $M_i$. Said another way:

\begin{lemma}
If $M_i$ has no job at time $t$, then for all $i' > i$, $M_{i'}$ has no job at any time $t' > t$.
\end{lemma}

\begin{proof}
The cyclic rule works by iterating through each timestep, and within each timestep, iterating through each machine. If at some time $t$ it is unable to assign a job to machine $M_i$, this means that all remaining chains have a link in that chain scheduled on an earlier machine at that timestep. It follows that no machine $M_{i'}$ with $i' > i$ will have a job scheduled at time $t$ either. This also implies that the number of remaining chains is less than $i$. Then at any time $t' > t$, the number of remaining chains is also less than $i$. All the chains will be exhausted before reaching $M_i$ at that timestep too.
\end{proof}

So the result of our scheduling algorithm is a kind of staircase, with earlier machines extending farther. Using the lemma below, we establish the internal structure of schedule outputs from our algorithm, by showing that after two ``steps'' in the staircase, each machine must only have jobs from the same chain.

\begin{lemma}
Let $t$ be the smallest time such that there exists a machine that has nothing scheduled at time $t$. If $M_1$ has a job from chain $C_j$ scheduled at time $t+1$, only jobs from chain $C_j$ are scheduled on $M_1$.
\end{lemma}
\begin{proof}
The fact that $M_1$'s job at $t+1$ is from $C_j$ implies that among the chains with the longest remaining length at the start of $t+1$, $C_j$ has the highest initial chain priority.

Since at time $t$, at least one machine is empty, a one-to-one assignment from remaining chains to machines is impossible, and so the number of remaining chains to be scheduled is less than the number of machines. Each chain that is present at $t$ has a link scheduled, so the length of each remaining chain at the start of time $t$ is one more than the length of those chains at time $t+1$.

We show by a kind of backwards induction that for all $t' \leq t$, $C_j$ is still the chain with the highest chain priority among the chains with longest length at time $t'$, and is therefore assigned on $M_1$ at time $t'$. Furthermore, every chain that finishes before $t$ has, at each timestep before $t$, at least 2 fewer links that $C_j$ remaining; and every chain finishing at $t$ has at least 1 fewer link than $C_j$ remaining.

This is true at time $t$: since the length of each chain present at time $t+1$ is one less than its length at $t$, the relative differences in their lengths are still the same, and so the longest chains at time $t$ are the same as those at time $t+1$. Within these chains, $C_j$ still has the highest initial chain priority. $C_j$ also has higher instant priority than the chains that finished at time $t$ because $C_j$ has at least 2 links at the start of time $t$, while the others have $1$. The chains that finish before $t$ have 0 chains remaining, 2 fewer than the $\geq 2$ chains on $C_j$. 

If this is true at the start of time $t' \leq t$, it is also true at $t'-1$. This is because no chain ending before $t$ could possibly be scheduled on a machine before $C_j$ at time $t'-1$; if such a chain $C_k$ did get scheduled before $C_j$, $C_k$ must have started $t'-1$ with \emph{at least} 1 fewer chain than $C_j$ (in the case $C_j$ doesn't get scheduled), since at the start of $t'$, $C_k$ has at least 2 fewer chains than $C_j$ by the inductive hypothesis. Even in this scenario, $C_j$ is shorter than $C_k$ at the start of $t'-1$, so it can't have been scheduled before $C_k$. The only chains that could be scheduled at $t'-1$ before $C_j$ are the chains ending after the start of $t$, of which there are fewer than $m$. $C_j$ is thus scheduled at time $t'-1$, so that its length at the start of $t'-1$ is one more than at the start of $t'$. Each chain $C_k$ can be scheduled at most once per timestep, so since $C_j$ is scheduled at $t'-1$, the amount by which $C_j$'s length exceeds the length of $C_k$ at $t'-1$ is at least as great as at $t'$, so that chains shorter than $C_j$ at $t'$ are shorter at $t'-1$, and among the chains of the same length as $C_j$ at $t'-1$, $C_j$ still has highest chain priority. Thus $C_j$ remains as having the highest instant priority at $t'-1$; it is scheduled on $M_1$.

The hypothesis is also true for $t' > t+1$, as going from $t'$ to $t'+1$ is just assigning the $i$-th highest priority chain to $M_i$; the differences between the chain lengths are preserved, so $C_j$ remains the highest priority chain at time $t'+1$. 
\end{proof}

From the induction, we can see that at $t = 0$, the chain $C_j$ scheduled on $M_1$ is in fact $C_1$, the one with highest chain priority and thus initial length. Since $M_1$'s schedule is contiguous and the longest, \textit{only} $M_1$ has links in $C_1$ assigned to it.

If we remove that chain from the input and use 1 fewer machine, we can see that the assignments on the remaining machines are the same as in the original instance, since assignment on machines $M_2, \ldots, M_m$ on the original input instance disregards the $C_1$ chain anyway and the schedule on $M_2, \ldots, M_m$ finishes before that on $M_1$.

From this fact, we can conclude that $M_2$ holds only chain $C_2$, and $M_3$ $C_3$, and so on, as long as the length of the chain on that machine is at least $t+1$. We call these machines and (largest) chains, for which each machine gets assigned only one chain, \textit{tower machines} and chains.

\begin{theorem} \label{thm:towers}
Let $t$ be the smallest time such that there exists a machine that has nothing scheduled at time $t$. If $M_i$ has a job scheduled at time $t+1$, $M_i$ has only and all jobs from chain $C_i$.
\end{theorem}

A stronger statement, that any machine with a job at time $t$ is a tower machine, does not hold, for example $M_2$ in Figure \ref{fig:cyclic_ex} above, though some machines with jobs at time $t$ could be tower machines, like $M_1$ in the same example.

The place in the above proof where such an assertion would break down is in the inductive step, where we show that $C_j$ is actually assigned to a machine at time $t'-1$; without the extra timestep exclusive to $C_j$, $C_j$ could exceed $C_k$ in length by just 1, and so at the start of $t'-1$, their lengths may be the same, with $C_k$ having a higher chain priority. We can't conclude any more that only chains ending after $t$ could possibly be scheduled before $C_j$.

\begin{corollary}
If chain $C_j$ has links assigned to different machines $M_a$ and $M_b$ with $a < b$, then the number of jobs assigned to $M_a$, $|M_a|$, is no more than $|M_b| + 1$.
\end{corollary}
\begin{proof}
If not, then $|M_a| \geq |M_b| + 2$. Then $M_b$ is empty at time $|M_b|+1$ but $|M_a|$ has a job at $|M_b|+2 \leq |M_a|$; by Theorem \ref{thm:towers} it must therefore contain all of $C_j$, contrary to $C_j$'s assignment on $M_b$.
\end{proof}

\begin{lemma}\label{tilemma}
If $C_j$ is scheduled on $M_i$ at time $t$ and $C_j$ is scheduled at time $t+1$ on $M_k$, then $k \geq i$. 
\end{lemma}
\begin{proof}
If $C_j$ is scheduled on $M_i$ at time $t$, $i-1$ other chains $C_{j_k}$ must have been scheduled on $M_k$ for $k \in [i-1]$, so that at the start of time $t$, each $C_{j_k}$ was either longer or of the same length but of higher chain priority than $C_j$. At the start of $t+1$, each $C_{j_k}$, as well as $C_j$, has decreased in length by 1; the relative differences in lengths are the same, so each $C_{j_k}$ would still go before $C_j$ in assignment if $C_j$ is scheduled at $t+1$; $C_j$ must therefore go after $i-1$ other jobs, so the earliest machine it could be assigned to is $M_i$.
\end{proof}

\begin{theorem}
Scaling each machine's schedule evenly to fit a single makespan does not violate precedence constraints, and therefore maintains a valid schedule.
\end{theorem}
\begin{proof}
None of the tower machines/chains will cause any precedence violations after scaling, because each of the chains is set on a single machine. There is some $\ell$ such that the remaining machines have length either $\ell$ or $\ell-1$, and due to the staircase schedule structure, the machine with the longer assignment must have an earlier index.

If in the unscaled schedule, job $c$ on machine $M_a$ at time interval $[t_c, t_c+1)$ is a parent of job $d$ on $M_b$ at $[t_d, t_d+1)$, then $t_c+1 \leq t_d$. If $|M_a| \geq |M_b|$, then composing the two inequalities, we get that the scaled end-time of $c$, $(t_c+1)/|M_a| \leq t_d/|M_b|$, the scaled start-time of $d$, so no precedence constraints are violated. Otherwise, $|M_a| = |M_b| - 1 = \ell-1$; in this case, $b < a$, and $t_d$ cannot be $t_c+1$ by Lemma \ref{tilemma}, so $t_d \geq t_c + 2$. We also have that $t_d+1 \leq \ell$, so $t_d/\ell < 1$, so that $t_d \geq t_c + 2 > t_c + 1 + t_d/\ell$. Rearranging this, we get
\[t_d\ell > t_d + \ell + t_c\ell \implies t_d(\ell - t_d) > t_c\ell + \ell \implies \frac{t_d}{\ell} > \frac{t_c+1}{\ell-1}\] which is the desired inequality expressing that the scaled start time of the child is after the scaled end time of the parent.
\end{proof}

\subsection{Optimality}

With knowledge of the structure of the schedules produced by the algorithm, it's tempting to think that the global optimality of this algorithm is trivial. The longest chains are given their own machines so that the jobs that \textit{need} to be short as a result of tight precedence constraints are given as much room as possible. The other chains are scheduled on the rest of the machines with the load almost perfectly evenly distributed. Yet, we had some difficulty proving this statement, and even that this cyclic rule would result in such an emergent structure was not obvious when we first played with algorithm ideas.

It is, however, easy to see by convexity that after fixing the machine assignment with the one produced by our algorithm, evenly scaling each machine's schedule to match a single makespan will minimize the energy. In fact, a stronger statement follows almost immediately. Each tower chain $C_j$ is minimal in its energy usage, since no matter the machine assignment, the sum of the lengths of each link on the chain must cannot exceed the makespan due to precedence constraints. Convexity says that in this case, the lowest energy results when each link in the chain has length $C_{max}/|C_j|$, which is what our algorithm does.

Further, the non-tower machines have jobs as evenly distributed between them as possible---as their length is either $\ell$ or $\ell+1$---so that among the non-tower chains/machines, our even scaling leads to the optimal energy schedule.

In a sense, our algorithm is optimal if a set---or just number---of tower machines $m_t$ is fixed beforehand. From there it's obvious that assigning the largest $m_t$ chains in the problem instance as the tower chains would be the most beneficial.

The optimality result we were able to prove is only for a single tower chain. The transformations in our exchange argument was not at all obvious to us initially. It relies on showing that given any schedule with just one tower chain, we can create a better one one that has the tower chain on its own machine. It relies on the following property of convexity.
\begin{fact}[Karamata]
\def\reals{\mathbb{R}}
Let $I$ be an interval. If $x_1, \ldots, x_n, y_1, \ldots, y_n \in I$ are such that $x_1 \geq \cdots \geq x_n$ and $y_1 \geq \cdots \geq y_n$ and $x$ majorizes $y$, i.e., for all $k \in [n]$,
\[\sum_{i=1}^k x_i \geq \sum_{i=1}^k y_i\]
then for any nondecreasing, convex $f : I \to \reals$, 
\[\sum_{i=1}^n f(x_i) \geq \sum_{i=1}^n f(y_i)\]
\end{fact}
\begin{lemma}[Swapping]\label{swap}
Suppose a schedule contains a $t_n$-duration block of $n$ unit-work jobs at uniform speed and elsewhere contains a $t_m$-long block of $m$ unit-work jobs at uniform speed. If $t_m \geq t_n$ and $n \geq m$, then swapping the two block durations, so that the $n$ block takes time $t_m$ and the $m$ takes time $t_n$, with the time in each block still split between jobs evenly, will create a new schedule using no more energy than the original.
\end{lemma}
\begin{proof}
Each job in the original $n$ block takes time $t_n/n$, and thus power $p(n/t_n)$, so the entire $n$ block originally takes $n\cdot p(n/t_n)$. Similarly, the $m$ block takes power $m\cdot p(m/t_m)$. After swapping, the new power required for $n$ is $n\cdot p(n/t_m)$ and for $m$, $m\cdot p(m/t_n)$.

If $n \geq m$ and $t_m \geq t_n$, then $n/t_n$ is greater than $n/t_m$, $m/t_n$, which are both greater than $m/t_m$. Then the sequence $x = (n/t_n, \ldots, n/t_n, m/t_m, \ldots, m/t_m)$ is nondecreasing and majorizes nondecreasing sequence $y$, defined as $(n/t_m, \ldots, n/t_m, m/t_n, \ldots, m/t_n)$ or the reverse, in case $m/t_n > n/t_m$. This is because for $i \in [n]$, $x_i = n/t_n$ exceeds every element in $y$, so $x$ majorizes $y$ up to index $n$. Furthermore, $x$ majorizes $y$ at $m+n$:
\[\sum_{i=1}^{m+n}x_i = \frac{n^2}{t_n} + \frac{m^2}{t_m} \geq \frac{m^2}{t_n} + \frac{n^2}{t_m} = \sum_{i=1}^{m+n}y_i\]
as $n^2(1/t_n - 1/t_m) \geq m^2(1/t_n - 1/t_m)$ for $n \geq m$ and $t_m \geq t_n$. Since each $x_{n+i} = m/t_m$ for $i \in [m]$ is smaller than anything in $y$, the difference $x_{n+i} - y_{n+i}$ is negative. If at any point $i$ between $n$ and $n+m$, $x$ does not majorize $y$, the negative difference would make $x$ to not majorize $y$ at $i+1$, then $i+2$, and so on. Since $x$ majorizes $y$ at the end, $x$ must majorize $y$ at all values. By Karamata,
\[\sum_{i=1}^{n+m}p\p*{x_i} = \sum_{i=1}^n p\p*{\frac{n}{t_n}} + \sum_{i=n+1}^{n+m} p\p*{\frac{m}{t_m}} = n\cdot p\p*{\frac{n}{t_m}} \geq \sum_{i=1}^{n+m}p\p*{y_i} = n\cdot p\p*{\frac{n}{t_m}} + m\cdot p\p*{\frac{m}{t_n}}\]
\end{proof}

\begin{figure}[h!]
\begin{center}
\begin{tabular}{lcr}
\begin{tikzpicture}[baseline=10]
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\draw (0.3,0.5) rectangle (1.6,1.5) node[pos=.5] {A};
\draw (1.6,0.5) rectangle (2.9,1.5) node[pos=.5] {A};
\draw (0.3,1.5) rectangle (1,2.5) node[pos=.5] {B};
\draw (1,1.5) rectangle (1.7,2.5) node[pos=.5] {B};
\draw (1.7,1.5) rectangle (2.4,2.5) node[pos=.5] {B};

\end{tikzpicture} \hspace{1cm} &
\begin{tikzpicture}[baseline=10]
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\draw (0.3,1.5) rectangle (1.35,2.5) node[pos=.5] {B};
\draw (1.35,1.5) rectangle (2.4,2.5) node[pos=.5] {B};
\draw (0.3,0.5) rectangle (1.17,1.5) node[pos=.5] {A};
\draw (1.16,0.5) rectangle (2.03,1.5) node[pos=.5] {A};
\draw (2.03,0.5) rectangle (2.9,1.5) node[pos=.5] {A};

\end{tikzpicture}
\end{tabular}
\end{center}
\caption{Swapping example}
\label{fig:swap_ex}
\end{figure}

Basically swapping a block tighter in both work and time with another that has more time and less work will reduce energy, with some constraints. Frustratingly, a stronger version of the lemma that doesn't require the block to run at uniform speed doesn't seem to hold in general, for example if $n$ has 2 jobs taking time $1/2$ each and $m$ has 2 jobs taking time $3/2$ and $1/2$, and $p(x) = x^2$. The original schedule would take energy $2^2 + 2^2 + (2/3)^2 + 2^2 \approx 12.44$, while the new one would take $1^2 + 1^2 + (4/3)^2 + 4^2 \approx 19.78$. Fortunately, this stronger version is not necessary for our proof.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{lcr}
\begin{tikzpicture}[baseline=10]
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\draw (0.3,0.5) rectangle (1.6,1.5) node[pos=.5] {M};
\draw (1.6,0.5) rectangle (2.9,1.5) node[pos=.5] {M};
\draw (0.3,1.5) rectangle (1.6,2.5) node[pos=.5] {N};
\draw (1.6,1.5) rectangle (5.5,2.5) node[pos=.5] {N};

\end{tikzpicture} \hspace{1cm} &
\begin{tikzpicture}[baseline=10]
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\draw (0.3,0.5) rectangle (0.95,1.5) node[pos=.5] {N};
\draw (0.95,0.5) rectangle (2.9,1.5) node[pos=.5] {N};
\draw (0.3,1.5) rectangle (2.9,2.5) node[pos=.5] {M};
\draw (2.9,1.5) rectangle (5.5,2.5) node[pos=.5] {M};

\end{tikzpicture}
\end{tabular}
\end{center}
\caption{Swapping failure}
\label{fig:swap_fail}
\end{figure}

\begin{lemma}[Flattening]\label{flat} If a schedule block of length $t$ for one machine contains unit-work jobs, the same schedule that rescales the block so that the jobs run at a uniform speed but total block time is the same does not increase the schedule's energy use.
\end{lemma}
\begin{proof}
Suppose the block contains jobs with runtimes $t_1, \ldots, t_n$ summing to $t$, with $t_1 \leq t_2 \leq \cdots \leq t_n$ (so the times are sorted by length, not by when the job starts). The total energy use is $\sum_{i=1}^n p(1/t_i)$. If the jobs are rescaled so that they all run at the same rate, then each job would take time $t/n$, and the total energy use would be $\sum_{i=1}^n p(n/t)$. Since the uniform sequence $\{n/t\}_{i=1}^n$ is majorized by every other nonincreasing sequence summing to the same total, including $\{1/t_i\}_{i=1}^n$, the new energy sum is less than the former, by Karamata.
\end{proof}

These two energy-preserving transformations let us turn schedules into better ones that don't have tower chains scheduled across machines.

\begin{lemma}\label{singmach}
Given an energy schedule on jobs $a_1, \ldots, a_n$ and $b_1, \ldots, b_m$ such that $a_1 \prec a_2 \prec \cdots \prec a_n$---and no precedence constraints on $b_i$---there exists a schedule using no more energy than this original schedule with all $a_1, \ldots, a_n$ on a single machine.
\end{lemma}
\begin{proof}
The general procedure for this starts at $a_1$ and finds a way to put it on the same machine as $a_2$, directly underneath it. It then finds a way to put the now-contiguous $a_1a_2$ block and puts it under $a_3$, and so on. Formally, let the schedule that respects precedence constraints be $S_1$, and from $S_i$ we can construct $S_{i+1}$ by finding the contiguous block of $c_i = a_1\cdots a_i$ in $S_i$ and modifying $S_i$ so that $c_i$ is placed underneath $a_{i+1}$ in a way that doesn't increase energy use and preserves precedence. Let $j_1, \ldots, j_k$ be the jobs scheduled before $a_{i+1}$ on $a_{i+1}$'s machine, where $b_{j_1}$ is scheduled before $b_{j_2}$, and so on. We have two cases:

If $k \geq i$, there are at least as many jobs under $a_{i+1}$ as in $a_1\cdots a_i$, and we can simply swap $a_i$ with $j_k$, $a_{i-1}$ with $j_{k-1}$, and so on until $a_1$ with $j_{k-i+1}$. The distribution of speeds across machines is still the same, so energy is preserved. Furthermore, no precedence constraints within $a_1, \ldots, a_i$ or between this block and $a_{i+1}, \ldots, a_n$ have been broken because they are all in sequence underneath $a_{i+1}$; nothing within $a_{i+1}, \ldots, a_n$ has broken either because we didn't change any of those jobs' positions. This is our $S_{i+1}$. See Figure \ref{fig:swap_1}. 

\begin{figure}[h!]
\begin{center}
\begin{tabular}{lcr}
\begin{tikzpicture}[baseline=10]
\def\w{0.5}
\def\y{0.5}
\def\x{0.3}
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\node at (0,3) {M$_3$};
\draw (\x,\y) rectangle ({\x+2*\w},{\y+1}) node[pos=.5] {B};
\draw ({\x+2*\w},\y) rectangle ({\x+5*\w},{\y+1}) node[pos=.5] {C};
\draw ({\x+5*\w},\y) rectangle ({\x+7*\w},{\y+1}) node[pos=.5] {D};
\draw ({\x+7*\w},\y) rectangle ({\x+9*\w},{\y+1}) node[pos=.5] {A$_3$};
\draw ({\x+9*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {F};
\def\y{1.5}
\draw (\x,\y) rectangle ({\x+4*\w},{\y+1}) node[pos=.5] {A$_1$};
\draw ({\x+4*\w},\y) rectangle ({\x+6*\w},{\y+1}) node[pos=.5] {A$_2$};
\draw ({\x+6*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {E};
\def\y{2.5}
\draw (\x,\y) rectangle ({\x+8*\w},{\y+1}) node[pos=.5] {G};
\draw ({\x+8*\w},\y) rectangle ({\x+10*\w},{\y+1}) node[pos=.5] {H};
\draw ({\x+10*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {A$_4$};

\end{tikzpicture} \hspace{1cm} &
\begin{tikzpicture}[baseline=10]
\def\w{0.5}
\def\y{0.5}
\def\x{0.3}
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\node at (0,3) {M$_3$};
\draw (\x,\y) rectangle ({\x+2*\w},{\y+1}) node[pos=.5] {B};
\draw ({\x+2*\w},\y) rectangle ({\x+5*\w},{\y+1}) node[pos=.5] {A$_1$};
\draw ({\x+5*\w},\y) rectangle ({\x+7*\w},{\y+1}) node[pos=.5] {A$_2$};
\draw ({\x+7*\w},\y) rectangle ({\x+9*\w},{\y+1}) node[pos=.5] {A$_3$};
\draw ({\x+9*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {F};
\def\y{1.5}
\draw (\x,\y) rectangle ({\x+4*\w},{\y+1}) node[pos=.5] {C};
\draw ({\x+4*\w},\y) rectangle ({\x+6*\w},{\y+1}) node[pos=.5] {D};
\draw ({\x+6*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {E};
\def\y{2.5}
\draw (\x,\y) rectangle ({\x+8*\w},{\y+1}) node[pos=.5] {G};
\draw ({\x+8*\w},\y) rectangle ({\x+10*\w},{\y+1}) node[pos=.5] {H};
\draw ({\x+10*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {A$_4$};

\end{tikzpicture}
\end{tabular}
\end{center}
\caption{More jobs under the next link than in the first part of the chain. Swap each $A$ one by one.}
\label{fig:swap_1}
\end{figure}

Otherwise, there are $k < i$ jobs under $a_{i+1}$. In this case, we can first flatten $c_i$ so that all $a_1, \ldots, a_i$ take the same amount of time. Lemma \ref{flat} guarantees that energy is not increased, and precedence is still preserved as $a_1, \ldots, a_i$ are still in sequence before $a_{i+1}$, as they were in the original $S_i$. We also flatten $j_1, \ldots, j_k$ which doesn't affect precedence (as $j_1, \ldots, j_k$ are disjoint from $a_1, \ldots, a_i$, or else $k \geq i$). Since $j_1\cdots j_k$ ends at the start of $a_{i+1}$, it's at least as long as $a_1\cdots a_i$, so Lemma \ref{swap} guarantees that swapping these two flattened blocks will not increase energy use. Like in the first case, precedence is preserved because nothing above or at $a_{i+1}$ has changed, and $a_1, \ldots, a_i$ are now in sequence underneath $a_{i+1}$. See Figure \ref{fig:swap_2}.

\begin{figure}[h!]
\begin{center}
\begin{tabular}{lcr}
\begin{tikzpicture}[baseline=10]
\def\w{0.3}
\def\y{0.5}
\def\x{0.3}
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\node at (0,3) {M$_3$};
\draw (\x,\y) rectangle ({\x+2*\w},{\y+1}) node[pos=.5] {B};
\draw ({\x+2*\w},\y) rectangle ({\x+5*\w},{\y+1}) node[pos=.5] {A$_1$};
\draw ({\x+5*\w},\y) rectangle ({\x+7*\w},{\y+1}) node[pos=.5] {A$_2$};
\draw ({\x+7*\w},\y) rectangle ({\x+9*\w},{\y+1}) node[pos=.5] {A$_3$};
\draw ({\x+9*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {F};
\def\y{1.5}
\draw (\x,\y) rectangle ({\x+4*\w},{\y+1}) node[pos=.5] {C};
\draw ({\x+4*\w},\y) rectangle ({\x+6*\w},{\y+1}) node[pos=.5] {D};
\draw ({\x+6*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {E};
\def\y{2.5}
\draw (\x,\y) rectangle ({\x+8*\w},{\y+1}) node[pos=.5] {G};
\draw ({\x+8*\w},\y) rectangle ({\x+10*\w},{\y+1}) node[pos=.5] {H};
\draw ({\x+10*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {A$_4$};

\end{tikzpicture} \hspace{1cm} &
\begin{tikzpicture}[baseline=10]
\def\w{0.3}
\def\y{0.5}
\def\x{0.3}
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\node at (0,3) {M$_3$};
\draw (\x,\y) rectangle ({\x+2*\w},{\y+1}) node[pos=.5] {B};
\draw ({\x+2*\w},\y) rectangle ({\x+4.33*\w},{\y+1}) node[pos=.5] {A$_1$};
\draw ({\x+4.33*\w},\y) rectangle ({\x+6.67*\w},{\y+1}) node[pos=.5] {A$_2$};
\draw ({\x+6.67*\w},\y) rectangle ({\x+9*\w},{\y+1}) node[pos=.5] {A$_3$};
\draw ({\x+9*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {F};
\def\y{1.5}
\draw (\x,\y) rectangle ({\x+4*\w},{\y+1}) node[pos=.5] {C};
\draw ({\x+4*\w},\y) rectangle ({\x+6*\w},{\y+1}) node[pos=.5] {D};
\draw ({\x+6*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {E};
\def\y{2.5}
\draw (\x,\y) rectangle ({\x+5*\w},{\y+1}) node[pos=.5] {G};
\draw ({\x+5*\w},\y) rectangle ({\x+10*\w},{\y+1}) node[pos=.5] {H};
\draw ({\x+10*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {A$_4$};

\end{tikzpicture} \hspace{1cm} &
\begin{tikzpicture}[baseline=10]
\def\w{0.3}
\def\y{0.5}
\def\x{0.3}
\node at (0,1) {M$_1$};
\node at (0,2) {M$_2$};
\node at (0,3) {M$_3$};
\draw (\x,\y) rectangle ({\x+2*\w},{\y+1}) node[pos=.5] {B};
\draw ({\x+2*\w},\y) rectangle ({\x+5.5*\w},{\y+1}) node[pos=.5] {G};
\draw ({\x+5.5*\w},\y) rectangle ({\x+9*\w},{\y+1}) node[pos=.5] {H};
\draw ({\x+9*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {F};
\def\y{1.5}
\draw (\x,\y) rectangle ({\x+4*\w},{\y+1}) node[pos=.5] {C};
\draw ({\x+4*\w},\y) rectangle ({\x+6*\w},{\y+1}) node[pos=.5] {D};
\draw ({\x+6*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {E};
\def\y{2.5}
\draw (\x,\y) rectangle ({\x+3.33*\w},{\y+1}) node[pos=.5] {A$_1$};
\draw (\x+3.33*\w,\y) rectangle ({\x+6.66*\w},{\y+1}) node[pos=.5] {A$_2$};
\draw ({\x+6.66*\w},\y) rectangle ({\x+10*\w},{\y+1}) node[pos=.5] {A$_3$};
\draw ({\x+10*\w},\y) rectangle ({\x+12*\w},{\y+1}) node[pos=.5] {A$_4$};

\end{tikzpicture}
\end{tabular}
\end{center}
\caption{Not enough jobs under the next $A$. First flatten the blocks to swap, then swap together.}
\label{fig:swap_2}
\end{figure}

Then $S_n$ has all $a_1, \ldots, a_n$ in a single block on the machine that $a_n$ was on in $S_1$. Since $S_{i+1}$ never uses more energy than $S_i$, $S_n$ doesn't use more energy than $S_1$.
\end{proof}

\begin{theorem}
Any scaled schedule from the cyclic rule containing at most 1 tower chain is optimal.
\end{theorem}
\begin{proof}
Given an input instance for which the cyclic rule produces one tower chain $a_1, \ldots, a_n$ and a schedule $S$ that is optimal, construct an $S'$ using Lemma \ref{singmach} that puts all of $a$ on a single machine $M_j$. We further modify this schedule so that it has only $a_1, \ldots, a_n$ on $M_1$, though it may break precedence on chains other than $a$.

If there are fewer jobs on $M_1$ than $n$ in $S'$, flatten the entire $M_1$ schedule, flatten $a_1\cdots a_n$, and swap the two blocks so that only $a_1, \ldots, a_n$ is on $M_1$. Otherwise, swap the latest job on $M_1$ with $a_n$, the 2nd with $a_{n-1}$, and so on so that $a_1, \ldots, a_n$ are in sequence on $M_1$ but there may be some jobs $j_1, \ldots, j_k$ underneath.

Flatten all machine schedules in $S'$; this doesn't violate precedence in $a$ because all $a_i$ are in sequence on $M_1$. Finally, for each $i \in [k]$, move $j_i$ to the a machine in $M_2, \ldots, M_m$ with fewest jobs, and flatten all machine schedules of $S'$ again. To show that each step conserves energy, we just need to show that we're always moving from a machine with more jobs to a machine with fewer. (A more rigorous argument would use the fact that if one machine had $a$ jobs and another had $b > a$, then $a^{-1}, \ldots, a^{-1}, b^{-1}, \ldots, b^{-1}$ majorizes $(a+1)^{-1}, \ldots, (a+1)^{-1}, (b-1)^{-1}, \ldots, (b-1)^{-1}$).

Let $n'$ be the number of jobs not in $a$ and $m$ the number of machines. At least one machine in $M_2, \ldots, M_m$ in our current $S'$ has at most $\ceil{n'/(m-1)}$ jobs, or else the total number of jobs on $M_2, \ldots, M_m$ would exceed $(m-1)(\ceil{n'/(m-1)} + 1) \geq n' + 1$, implying one of these machines has a job in $a$, contrary to all of $a$ being on $M_1$. Since $a$ is a tower chain, $n$ exceeds $\ceil{n'/(m-1)}$, because each non-tower machine in a \emph{cyclic rule} schedule differs from each other in number of jobs by at most 1, so the job numbers on those machines are $\floor{n'/(m-1)}$ or $\ceil{n'/(m-1)}$, where at least one machine has $\floor{n'/(m-1)}$; and then a tower machine has at least 2 more jobs than that. Thus $M_1$, having more than $n > \ceil{n'/(m-1)}$, is always moving jobs to a machine with at most $\ceil{n'/(m-1)}$ jobs. 

Then an optimal schedule $S$ exists in which: $M_1$ has all and only jobs from $a$, each machine runs at a uniform speed, and precedence in chain $a$ (but possibly not the others) is preserved.

We claim that the cyclic rule output $C$ is at least as good as this final $S$. The energy $M_1$ uses on both $S$ and $C$ is the same. Among $M_2, \ldots, M_m$, $C$ and $S$ are also flat, but somehow $C$ distributes the remaining jobs maximally evenly, putting $\ell + 1$ jobs on some machines and $\ell$ on others. We can use Karamata one more time: if a machine schedule is completely flat and has $k$ jobs, the total energy used by that machine is $q(k) = k\cdot p(1/k)$. For second-differentiable $p$, $q''(k) = p''(1/k)/k^3$, which is nonnegative for $k > 0$, since $p$ is convex. Thus $q$ is convex. Any sequence $y$ with the same sum majorizes $x = (\ell+1, \ldots, \ell + 1, \ell, \ldots, \ell)$, so if $y$ is the sequence of job quantities on $M_2, \ldots, M_m$ in $S$, $q(y_1) + \cdots, q(y_{m-1}) \geq q(x_1) + \cdots + q(x_{m-1})$, the energy used by $M_2, \ldots, M_m$ in $C$.

Since $C$ uses at most as much energy as $S$ and is also valid precedence-wise, $C$ is optimal.
\end{proof}

Unfortunately, where this construction seems to breaks down is with multiple tower chains. One keystone of this argument is that in transforming a schedule into one with a tower chain solely on one machine, precedence for other jobs don't matter. This is a valid assumption when there's at most 1 tower chain because given the job quantities on the other non-tower machines, the actual job assignments can be set so as to preserve precedence. If there are multiple tower chains, naive extensions to the swapping transformation on one chain seem to break precedence on other chains, and a similar naive restriction on the swaps so that chains don't interfere with each other makes it sometimes impossible to swap \emph{and} conserve energy. A different approach may be necessary here.

We were confident even before proving optimality for a single tower chain that this algorithm is optimal. Now, we know that given an input and an identification of certain chains as towers (through the cyclic rule), the optimal schedule for a subinput with only 1 tower and all the non-towers just puts the tower on 1 machine and spreads the non-towers over the others; i.e., the behavior of the non-towers doesn't change when you add a machine and tower chain. It seems just out of reach that this property holds when we add a second, or third, or fourth machine/chain.

\section{Trees}
The same question in the chains discussion applies in this more general class of precedence constraints. Hu's original $C_{max}$ algorithm \cite{hu} looks just as promising applied to trees as applied to chains, but for trees, complete algorithm specification---i.e., deciding how to break ties---is even more important in this larger space of problems, where the optimal algorithm obviously cannot simply scale each unit-work job uniformly (as our cyclic rule happened to be able to do in the chains case) but must respect precedence constraints that depend on each job's machine assignment. Our algorithm simply selects machine assignments for jobs and lets the convex optimizer decide the time boundaries. In this section, we present several heuristics for Hu's algorithm applied to trees, provide justification for their merits, and experiment each algorithm flavor on a corpus of trees.

\subsection{Heuristics}
\subsubsection{Cyclic Rule}
One promising heuristic is the cyclic rule generalized for trees, described below:
\begin{enumerate}
\item Given a node on a tree, sort each child by descending subtree depth, so that initially longer subtrees have higher \emph{local subtree priority}. If two nodes' have the same initial length, local subtree priority tie-breaking is arbitrary.
\item A node $A$ has a higher \emph{global subtree priority} than $B$ if $A$ is a descendant of $B$ or if the child of the lowest common ancestor of $A$ and $B$ containing $A$ has a higher local subtree priority than the child of the LCS of $A$ and $B$ containing $B$.
\item Number each node in the tree with the dictionary order, first by depth of the node, and then by global subtree priority. This is a node's \emph{node priority}.
\item Fix an order on the machines and iterate through them in that order each . To select a node to put on a machine, simply select the node with the node priority that doesn't have a node in its subtree scheduled at the same time.
\end{enumerate}

You can think of this as shifting the tree in 2-dimensional space so that each node has its longest subtrees below it on the right. Then a node's node priority is just its depth, then how far to the right it is.

What's appealing about this heuristic is that is naturally extends the heuristic for chains. Note that if the tree is a set of chains attached to a single root, then the global subtree priority corresponds to chain priority (the ordering relation between nodes in a chain instead of chains themselves) and node priority to link priority. When applied to chains, the cyclic rule had a nice property of minimizing the interference of precedence and scaling, allowing us to scale optimally. Though it's a toy example, the same behavior is apparent in Figure \ref{fig:cyct_ex}.

Furthermore, when the number of nodes at a particular depth is either much greater or much less than the number of machines, the behavior is similar to chains in general: if there are many nodes, nodes neighboring each other in the schedule will unlikely have any precedence relation so that even scaling is likely. If there are many machines, the rule simply schedules all the nodes at a particular depth at once and move up, similar to the tower behavior for chains.

\begin{figure}[h!]
\input{cyct_ex.tikz}
\caption{A cyclic rule example for trees. The number is the node priority.}
\label{fig:cyct_ex}
\end{figure}

\subsubsection{Zigzag Rule}
Another possible choice is to iterate through the machines in backwards order every other timestep. This is appealing because it tries to distribute the number of jobs more evenly among the machines, rather than always pile a job on the first machine if it's available.

We suspected that this was strictly worse than the cyclic rule due to its poor performance on chains, and we had even hoped the cyclic rule would turn out to be optimal, but our experiments refuted that. It's hard to find an intuitive example, since the interesting behavior seems to be in some Goldilocks zone, in which the number of bottom-level nodes left to be scheduled is around the same as the number of machines, but one counterexample is given in Figure \ref{fig:zz_ex}.

\begin{figure}[h!]
\input{zz_ex.tikz}
\caption{An example in which zigzag (middle) outperforms cyclic (right).}
\label{fig:zz_ex}
\end{figure}

\subsubsection{Random}
The random heuristic orders subtrees randomly, instead of performing the sort by local subtree priority in the cyclic rule. Additionally, in each round, it assigns jobs to machines in a random order. These changes make it so that every arbitrary choice in Hu's algorithm is now made uniformly at random.

For the random heuristic to perform well, it would have to be the case that it would be relatively easy to choices close to optimally; i.e. most random decisions should not raise the energy too much. If there are very few bad choices, we could even hope that, if repeated enough times, the random heuristic makes all the best choices and output the optimal algorithm of this type.

\subsection{Approximation Algorithm}
Here, we restrict to the case of polynomial power consumption, i.e. $p(s) = s^\alpha$ for $\alpha \ge 2$. This allows us to apply the power mean inequality instead of appealing to convexity, making a simple proof and a concrete bound. The relevant case of the inequality is stated here, and is a corollary to Jensen's inequality.

\begin{fact}[Power Mean]
For any positive $a_1, \dots, a_k$ and $\alpha \ge 1$:
\[ \frac{\sum_i a_i}{k} \le \p*{\frac{\sum_i a_i^\alpha}{k}}^{1/\alpha} \text{  or equivalently  } \p*{\sum_i a_i}^\alpha \le k^{\alpha - 1} \sum_i a_i^\alpha.\]
\end{fact}

In deriving their own approximation algorithm for $R|tree|C_{max}$, Kumar et. al. presents the following tool \cite{kumar}.
A chain decomposition of a DAG is a series of blocks $B_1 \dots B_\lambda$ each containing chains $\ell_1 \dots \ell_{n_{i}}$ with the following guarantee:
if $i \prec j$, then either \begin{enumerate}
    \item $i \in B_a$ and $j \in B_b$ such that $a < b$ or
    \item $i, j \in \ell$ and within the chain $i \prec j$
\end{enumerate}
Thus, if we have a chain decomposition of a DAG, we can always schedule blocks sequentially as long as we respect the chains within them.

For trees, it is fairly obvious how to create a chain decomposition. For a formal iterative algorithm, see the paper or our source code.
Here is a mathematical description of the decomposition, which gives bound on $\lambda$.
Quotient the tree by the following equivalence relation: $v \sim u$ if $u$ is the only child of $v$ or $u$'s only child is $v$, or $u = v$.
Clearly the equivalence classes are chains. Additionally, there are precedence relations between equivalence classes.
By construction, each equivalence class has either 0 or 2 or more children, giving us a tree with degree always at least 2.
Thus the height of the tree is at most $\log n$. Letting block $B_i$ be the set of chains at height (leaves are height 1) $i$, we conclude $\lambda \le \log n$.
In fact, this is the best lower bound for $\lambda$ in terms of $n$, as it is matched by the case where the starting tree is a complete binary tree.

\begin{figure}[h!]
\input{chain_decompose.tikz}
\caption{A demonstration of \textsc{ChainDecompose}. On the right is the original tree, with each equivalence class in a different color. Next, we show the quotient tree on the classes. Finally, we divide the levels of the quotient tree into blocks.}
\label{fig:chain_decompose}
\end{figure}

The approximation algorithm (Algorithm \ref{alg:approx_tree_sched}) uses our chain-scheduling algorithm on each block, then scales the blocks to evenly distribute jobs on the busiest (1st by construction) machine.

\begin{algorithm}
    \caption{An $O\p*{2^{2\alpha-1}m\p*{\log^{\alpha-1}n}}$ approximation algorithm for $P|tree,C_{max} = C|E$}
    \label{alg:approx_tree_sched}
  \begin{algorithmic}[1]
    \Require{$n$ and $m$ integers, $\prec$ a tree on $[n]$}
    \Function{ApproximateTreeSchedule}{$n$, $m$, $\prec$}
        \Let{$\lambda, B$}{\Call{ChainDecompose}{n, $\prec$}}
        \Let{$d$}{$0$}
        \Letn{$p$}{$0$}
        \For{$i \in [\lambda]$}
            \Let{$S_i, J_i$}{\Call{ChainSchedule}{$B_i$}}
            \Let{$d$}{$d$ $+$ length($J_{i,1}$)}
        \EndFor
        \For{$i \in [\lambda]$}
            \Let{$c$}{$C $ length($J_{i, 1}$)$/d$}
            \For{$a \in [m]$}
                \For{$j \in J_{a,i}$}
                    \Let{$S'_j$}{$p + cS_{i,j}$}
                    \State append $j$ to $J'_a$
                \EndFor
                \Let{$p$}{$p + c$}
            \EndFor
        \EndFor
        \State \Return{$S', J'$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{figure}[h!]
\begin{center}
\input{approx.tikz}
\end{center}
\caption{\textsc{ApproximateTreeSchedule} on the tree in Figure \ref{fig:chain_decompose} ($E = 780$) with blocks $B_i$ labeled.}
\end{figure}

\begin{figure}[h!]
\begin{center}
\input{better.tikz}
\end{center}
\caption{A better schedule for the tree in Figure \ref{fig:chain_decompose} ($E = 448$). Note that chains from different blocks are run simultaneously, allowing better performance than the approximation.}
\end{figure}

Clearly, this is an efficient algorithm in $n$ and $m$.
Now we bound the multiplicative approximation ratio $\rho \ge E / E^\star$ using the lower bounds established in section \ref{lb}. Note any chain scheduling algorithm produces a valid tree schedule when used in this manner; we use an additional fact about \textsc{ChainSchedule} in our analysis: the busiest machine always runs a number of jobs equal to the maximum of the longest chain length and the ceiling of the per-machine load, as proven in Theorem \ref{thm:towers}.

\begin{theorem} \label{thm:approx}
Algorithm \ref{alg:approx_tree_sched} is a $\rho \in O\p*{2^{2\alpha-1}m\p*{\log^{\alpha-1}n}}$ approximation algorithm for $P|tree, C_{max} = C|E$.
\end{theorem}
\begin{proof}
Consider the busiest machine in subschedule $i$. If the machine is occupied by a ``tower chain'', we have a load $L_i \le k^i_{max}$. Otherwise, it is occupied by ``contention chains'' giving $L_i \le \ceil{\frac{n_i}{m}}$ where $n_i$ is the number of jobs in the block. Then clearly, the sum of these two quantities is a lower bound for $L_i$. Summing over blocks we get the load on the busiest machine.
\[L := \sum_{i \in [\lambda]} L_i \le \sum_{i \in [\lambda]} \ceil*{\frac{n_i}{m}} + \sum_{i \in [\lambda]} k^i_{max}.\]
In place of the longest chain in each block, we can upper bound by the longest disjoint chains in the tree.
\[L \le \frac n m + \lambda + \sum_{i \in [\lambda]} k_{(i)}.\]
Additionally, each $k_{(i)} \ge 1$, so $\lambda \le \sum_{i \in [\lambda]} k_{(i)}$.
\[L \le \frac n m + 2\sum_{i \in [\lambda]} k_{(i)}.\]
Because we assign each of these jobs the same time period, we get the following power consumption for the jobs on the busiest machine.
\[E_1 \le C \p*{\frac{n}{mC} + 2\sum_{i \in [\lambda]} \frac{k_{(i)}}C}^{\alpha}.\]
Applying the power mean inequality for $k = 2$ brings the exponent inside the sum.
\[ E_1 \le 2^{\alpha - 1} \p*{C\p*{\frac{n}{mC}}^\alpha + \:2^\alpha C\p*{\sum_{i \in [\lambda]} \frac{k_{(i)}}{C}}^\alpha}.\]
We see the first term corresponds to Equation \ref{eq:lb1}, the contention lower bound.
\[ C\p*{\frac{n}{mC}}^\alpha \le \frac{E^\star}{m}.\]
Applying the power mean inequality, we see the second term corresponds to Equation \ref{eq:lb2}, the dilation lower bound.
\[ 2^\alpha C\p*{\sum_{i \in [\lambda]} \frac{k_{(i)}}{C}}^\alpha 
\le 2^\alpha \p*{\log^{\alpha-1}n} \: C\sum_{i \in [\lambda]}\p*{\frac{k_{(i)}}{C}}^\alpha 
\le 2^\alpha \lambda^{\alpha - 1} E^\star\]
Plugging these terms back into the expression for $E_1$,
we see
\[\frac{E_1}{E^\star} \le \frac{2^{\alpha-1}}m \p*{1 + 2^\alpha m \lambda^{\alpha-1}}.\]
Finally, on later machines, the load is smaller than $L$ and each job runs more slowly. Thus we can bound the total energy by multiplying $E_1$ by the number of machines.
\[\rho := \frac{E}{E^\star} \le 2^{\alpha - 1}\p*{1 + 2^\alpha m\lambda^{\alpha-1}}.\]
Dropping lower order terms,
\[\rho \in O\p*{2^{2\alpha-1}m\lambda^{\alpha-1}}.\]
As $\lambda \le \log n$, we have
\[\rho \in O\p*{2^{2\alpha-1}m(\log^{\alpha-1} n)}.\]
\end{proof}
This ratio increases drastically with $\alpha$, however, the most realistic setting is $\alpha = 3$ regardless of problem size. So in most applications $2^{2\alpha - 1} \approx 32$, a relatively small constant. Either of $m$ and $\log^{\alpha-1} n$ can dominate the expression, depending on their relative magnitudes. If $\lambda$ is smaller, i.e. the graph has fewer levels of chains, we note the algorithm performs better.

\subsection{Extension to Forests}
Consider a forest of in and out arborescences. Such a graph will have a chain decomposition with $\lambda \le 2 \p*{\ceil{\log n} + 1}$ (proof omitted) \cite{kumar}. Using this chain decomposition, we could simply run the same algorithm, and the analysis goes through almost identically, until we substitute in for the value of $\lambda$.

\begin{theorem}
Algorithm \ref{alg:approx_tree_sched} is a $\rho \in O\p*{2^{3\alpha-2}m\p*{\log^{\alpha-1}n}}$ approximation for $P|forests, C_{max} = C|E$.
\end{theorem}
\begin{proof}
Each step but the last in the proof of Theorem \ref{thm:approx} holds in this setting as well. Use the value $\lambda \le 2 \p*{\ceil{\log n} + 1}$ guaranteed for forests instead, giving the desired bound.
\end{proof}

\section{Experiment}

\subsection{Design}
Because of the inconclusiveness of our heuristics, we wanted to test their performance on real world data, as well as compare them to the approximation algorithm on the same data. We implemented each algorithm in Python, and the code is publicly available at
\[\texttt{https://github.com/kuykendall-benjamin/scheduling-algorithms/}\]
We used the CVXPY optimization library (authors Steven Diamond and Stephen Boyd) freely available at
\[\texttt{http://www.cvxpy.org/}\]
We used the English language data from the Universal Dependencies project, available at
\[\texttt{http://universaldependencies.org/}\]
For a set of trees, we used a corpus of written language dependencies. A dependency grammar is a syntactic model in linguistics that induces a certain tree on the words of a sentence. Thus the generated schedules represent recursively computing a function over a word's dependents for example. More generally, we hope these trees present a reasonable variety of test cases, with varying degree and height. They range from a few nodes to about one hundred; these are about the largest programs we can optimize quickly enough on personal computers.

\subsection{Results}
For each test case, we noted the number of machines, the number of jobs, the height of the tree, and the energy use. After dividing energy use by our lower bounds to get a metric that was close to the approximation ratio for each algorithm, we plotted this metric against the number of machines and the number of jobs, with error bars (abused---since the underlying data has its own variance---so that they represent) 1 standard deviation in one direction within the data. The plots are below.
\begin{figure}[h!]
\centering \includegraphics[scale=0.41]{jobs.eps}
\caption{Ratio vs. jobs, mean across number machines}
\end{figure}

\begin{figure}[h!]
\centering \includegraphics[scale=0.41]{machines.eps}
\caption{Ratio vs. machines}
\end{figure}

\begin{figure}[h!]
\centering \includegraphics[scale=0.41]{machines_noaa.eps}
\caption{Ratio vs. machines, excluding approximation results}
\end{figure}

We see a kind of logarithmic growth in metric for the approximation algorithm that agrees with the ratio we computed earlier. Along both dimensions, it seems that our cycling heuristic slightly outperforms random and zigzag on average no matter the number of machines or jobs, but surprisingly, random does worse than zigzag, despite zigzag being a kind of opposite to the heuristic. We were also shocked to see that there were many instances in which the cyclic heuristic was not optimal; one example is above in the zigzag heuristic section.

Another interesting thing is the relative stability of the metric for our 3 heuristics. Across all trees, no approximation ratio exceeds 4.5, and the graphs don't immediately suggest that increasing the number of jobs, machines, or height (not pictured) would steadily increase the ratio. Finding some way to bound the approximation ratio, or else finding a family of counterexamples, would be good next steps.

\renewcommand{\bibpreamble}{Our most used source was \cite{kumar} which gave us the tool of chain decompositions as a way to approximate schedules for trees and forests, and together with \cite{shmoys} helped us derive and discuss the relevant lower bounds. Additionally \cite{hu} presents the classic forest scheduling algorithms which served as a basis for our heuristics. The remaining papers are energy aware scheduling papers given on the syllabus, referred to only to help motivate the relevance and novelty of our problem definition. Throughout we use tools learned in class without citation.}
\break
\begin{thebibliography}{9}
\bibitem{kumar}
V. Kumar, M. Marathe, S. Parthasarathy, and A. Srinivasan. 
\textit{Scheduling on Unrelated Machines under Tree-Like Precedence Constraints},
Proc. International Workshop on Approximation Algorithms for Combinatorial Optimization Problems, 146-157, 2005.

\bibitem{hu}
T. C. Hu.
\textit{Parallel Sequencing and Assembly Line Problems},
Operations Research, Vol. 9, No. 6, 841-848, 1961.

\bibitem{shmoys}
D.B. Shmoys, C. Stein, and J. Wein.
\textit{Improved approximation algorithms for shop scheduling problems},
SIAM Journal on Computing, Vol. 23, 617-632, 1994.

\bibitem{yao}
F. Yao, A. Demers, and S Shenker.
\textit{A Scheduling Model for Reduced CPU Energy},
FOCS 36, 374, 1995.

\bibitem{bansal}
N. Bansal, H. Chan, and K. Pruhs.
\textit{Speed Scaling with an Arbitrary Power Function},
SODA 20, 693-701, 2009.

\bibitem{irani}
S. Irani, S. Shukla, and R. Gupta.
\textit{Algorithms for Power Savings},
ACM TALG, Vol. 3, Issue 4, Nov 2007.

\bibitem{baptiste}
P. Baptiste, M. Chrobak, C. Durr.
\textit{Polynomial Time Algorithms for Minimal Energy Scheduling},
arXiv, 0908.3505, 24 Aug 2009.
\end{thebibliography}

\end{document}

